#!/usr/bin/env python3
"""
Bilibiliè§†é¢‘å­—å¹•æå–å™¨
ä½¿ç”¨AITransDubç½‘ç«™æå–Bilibiliè§†é¢‘å­—å¹•
"""

import logging
import time
from bs4 import BeautifulSoup
from playwright.sync_api import sync_playwright
import re
from typing import List, Dict, Optional

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('subtitle_extractor.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class BilibiliSubtitleExtractor:
    def __init__(self):
        self.base_url = "https://www.aitransdub.com/bilibili-subtitles"
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        }
    
    def detect_video_platform(self, url: str) -> str:
        """æ£€æµ‹è§†é¢‘å¹³å°"""
        if 'bilibili.com' in url:
            return 'bilibili'
        elif 'youtube.com' in url or 'youtu.be' in url:
            return 'youtube'
        elif 'xiaohongshu.com' in url or 'xhs.app' in url:
            return 'xiaohongshu'
        else:
            return 'unknown'
    
    def extract_bilibili_subtitles(self, video_url: str) -> Dict:
        """ä»Bilibiliè§†é¢‘æå–å­—å¹•"""
        logger.info(f"å¼€å§‹æå–Bilibiliè§†é¢‘å­—å¹•: {video_url}")
        
        try:
            with sync_playwright() as p:
                # å¯åŠ¨æµè§ˆå™¨
                browser = p.chromium.launch(headless=True)
                page = browser.new_page()
                
                # è®¿é—®ç½‘ç«™
                logger.info("è®¿é—®AITransDubç½‘ç«™...")
                page.goto(self.base_url, timeout=30000)
                
                # è¾“å…¥è§†é¢‘URL
                logger.info("è¾“å…¥è§†é¢‘URL...")
                url_input = page.locator("input[placeholder='Paste video URL here']")
                url_input.clear()
                url_input.fill(video_url)
                
                # ç‚¹å‡»GenerateæŒ‰é’®
                logger.info("ç‚¹å‡»GenerateæŒ‰é’®...")
                generate_btn = page.locator("button:has-text('Generate')")
                generate_btn.click()
                
                # ç­‰å¾…é¡µé¢è·³è½¬åˆ°ç»“æœé¡µé¢
                logger.info("ç­‰å¾…é¡µé¢å¤„ç†...")
                # ç­‰å¾…URLå˜åŒ–æˆ–ç‰¹å®šå†…å®¹å‡ºç°
                try:
                    page.wait_for_url("**/video/**", timeout=90000)  # å¢åŠ åˆ°90ç§’
                except:
                    # å¦‚æœURLæ²¡æœ‰å˜åŒ–ï¼Œç­‰å¾…ä¸€æ®µæ—¶é—´è®©é¡µé¢å¤„ç†
                    logger.info("URLæœªå˜åŒ–ï¼Œç­‰å¾…æ›´é•¿æ—¶é—´...")
                    time.sleep(60)  # å¢åŠ åˆ°60ç§’
                
                current_url = page.url
                logger.info(f"é¡µé¢è·³è½¬åˆ°: {current_url}")
                
                # æ£€æŸ¥æ˜¯å¦æˆåŠŸè·³è½¬åˆ°è§†é¢‘é¡µé¢
                if '/video/' not in current_url:
                    raise Exception("é¡µé¢æœªè·³è½¬åˆ°è§†é¢‘é¡µé¢ï¼Œå¯èƒ½å¤„ç†å¤±è´¥")
                
                # å¯¹äºé•¿è§†é¢‘ï¼Œéœ€è¦ç­‰å¾…æ›´é•¿æ—¶é—´è®©å­—å¹•å®Œå…¨åŠ è½½
                logger.info("ç­‰å¾…å­—å¹•å†…å®¹å®Œå…¨åŠ è½½...")
                time.sleep(30)  # å¢åŠ ç­‰å¾…æ—¶é—´
                
                # å°è¯•æ»šåŠ¨é¡µé¢ä»¥è§¦å‘æ›´å¤šå†…å®¹åŠ è½½
                logger.info("æ»šåŠ¨é¡µé¢ä»¥åŠ è½½æ›´å¤šå†…å®¹...")
                for i in range(5):
                    page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                    time.sleep(3)
                    page.evaluate("window.scrollTo(0, 0)")
                    time.sleep(2)
                
                # è·å–é¡µé¢å†…å®¹
                page_content = page.content()
                browser.close()
                
                # è§£æå­—å¹•å†…å®¹
                subtitles = self._parse_subtitle_content(page_content, current_url)
                
                return {
                    'success': True,
                    'video_url': video_url,
                    'result_url': current_url,
                    'video_id': current_url.split('/video/')[-1],
                    'subtitles': subtitles,
                    'subtitle_count': len(subtitles)
                }
                
        except Exception as e:
            logger.error(f"å­—å¹•æå–å¤±è´¥: {str(e)}")
            return {
                'success': False,
                'error': str(e),
                'video_url': video_url
            }
    
    def _parse_subtitle_content(self, html_content: str, page_url: str) -> List[str]:
        """è§£æHTMLå†…å®¹æå–å­—å¹•æ–‡æœ¬"""
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # è·å–æ‰€æœ‰æ–‡æœ¬å†…å®¹
        all_texts = soup.find_all(text=True)
        logger.info(f"ç½‘é¡µæ€»å…±æ‰¾åˆ° {len(all_texts)} ä¸ªæ–‡æœ¬å…ƒç´ ")
        
        # å…ˆç”¨å®½æ¾çš„æ¡ä»¶æ”¶é›†æ‰€æœ‰å¯èƒ½çš„å­—å¹•
        potential_subtitles = []
        for text in all_texts:
            cleaned = text.strip()
            if len(cleaned) > 3:  # éå¸¸å®½æ¾çš„é•¿åº¦è¦æ±‚
                potential_subtitles.append(cleaned)
        
        logger.info(f"é•¿åº¦>3çš„æ–‡æœ¬æœ‰ {len(potential_subtitles)} ä¸ª")
        
        # ä¿å­˜è°ƒè¯•ä¿¡æ¯åˆ°æ–‡ä»¶
        debug_file = f"debug_content_{int(time.time())}.txt"
        with open(debug_file, 'w', encoding='utf-8') as f:
            f.write("=== æ‰€æœ‰å¯èƒ½çš„å­—å¹•å†…å®¹ ===\n\n")
            for i, text in enumerate(potential_subtitles[:100], 1):  # åªå†™å‰100ä¸ªé¿å…æ–‡ä»¶å¤ªå¤§
                f.write(f"[{i:03d}] {text}\n\n")
        
        logger.info(f"è°ƒè¯•ä¿¡æ¯å·²ä¿å­˜åˆ°: {debug_file}")
        
        # ç°åœ¨ç”¨æ›´å®½æ¾çš„è¿‡æ»¤æ¡ä»¶ï¼Œå¹¶å¯»æ‰¾çœŸæ­£çš„å­—å¹•å†…å®¹
        subtitle_texts = []
        
        # é¦–å…ˆç”¨éå¸¸å®½æ¾çš„æ¡ä»¶æ”¶é›†æ‰€æœ‰å¯èƒ½çš„å†…å®¹
        for text in potential_subtitles:
            if (len(text) > 5 and  # å¾ˆä½çš„é•¿åº¦è¦æ±‚
                not text.startswith(('[', '{', '<', '/*', 'function', 'window.', 'var ', 'const ', 'let ')) and
                not any(skip in text.lower() for skip in [
                    'class:', 'style:', 'onclick', 'href=', 'src=', 'datalayer',
                    'gtag', 'plausible', '-webkit-', '-moz-', 'rgba(', 'margin:',
                    'padding:', 'border:', 'background:', 'font-', 'color:',
                    'display:', 'position:', 'width:', 'height:', 'transition:'
                ]) and
                not re.search(r'px|em|rem|vh|vw|%|rgb|rgba|#[0-9a-fA-F]', text) and  # ä¸åŒ…å«CSSå•ä½
                not re.search(r'[{}();=]', text)):  # ä¸åŒ…å«ä»£ç ç¬¦å·
                
                subtitle_texts.append(text)
        
        logger.info(f"åˆæ­¥è¿‡æ»¤åæœ‰ {len(subtitle_texts)} æ®µå†…å®¹")
        
        # å¯»æ‰¾çœŸæ­£çš„å­—å¹•å†…å®¹å¼€å§‹ä½ç½®ï¼ˆé€šå¸¸æ˜¯è¿ç»­çš„ã€æœ‰æ„ä¹‰çš„å¥å­ï¼‰
        real_subtitle_start = 0
        chinese_count = 0
        
        for i, text in enumerate(subtitle_texts):
            # ç»Ÿè®¡åŒ…å«ä¸­æ–‡çš„è¿ç»­æ–‡æœ¬
            if re.search(r'[\u4e00-\u9fff]', text) and len(text) > 10:
                chinese_count += 1
                if chinese_count >= 3:  # è¿ç»­3ä¸ªä¸­æ–‡æ–‡æœ¬ï¼Œè®¤ä¸ºå¼€å§‹äº†çœŸæ­£çš„å­—å¹•
                    real_subtitle_start = max(0, i - 5)  # å¾€å‰å–5ä¸ªä½œä¸ºç¼“å†²
                    logger.info(f"æ£€æµ‹åˆ°çœŸæ­£çš„å­—å¹•å†…å®¹ä»ç¬¬ {real_subtitle_start} ä¸ªå¼€å§‹")
                    break
            else:
                chinese_count = 0
        
        # å¦‚æœæ²¡æ‰¾åˆ°ä¸­æ–‡å­—å¹•ï¼Œå°è¯•å¯»æ‰¾è‹±æ–‡å­—å¹•æ¨¡å¼
        if real_subtitle_start == 0:
            sentence_count = 0
            for i, text in enumerate(subtitle_texts):
                # å¯»æ‰¾åƒå¥å­çš„è‹±æ–‡å†…å®¹
                if (len(text) > 15 and 
                    len(text.split()) > 3 and
                    not any(nav in text.lower() for nav in ['transcript', 'download', 'subtitle', 'menu', 'sign', 'cookie']) and
                    ('.' in text or ',' in text or text[0].isupper())):
                    sentence_count += 1
                    if sentence_count >= 5:  # è¿ç»­5ä¸ªå¥å­æ ·å¼çš„æ–‡æœ¬
                        real_subtitle_start = max(0, i - 10)
                        logger.info(f"æ£€æµ‹åˆ°è‹±æ–‡å­—å¹•å†…å®¹ä»ç¬¬ {real_subtitle_start} ä¸ªå¼€å§‹")
                        break
                else:
                    sentence_count = 0
        
        # ä»æ£€æµ‹åˆ°çš„ä½ç½®å¼€å§‹æå–å­—å¹•
        final_subtitles = subtitle_texts[real_subtitle_start:]
        
        # æœ€åæ¸…ç†ï¼šç§»é™¤æ˜æ˜¾çš„ç½‘é¡µå…ƒç´ 
        cleaned_subtitles = []
        for text in final_subtitles:
            # è·³è¿‡æ˜æ˜¾çš„ç½‘é¡µå¯¼èˆªå’Œæ— å…³å†…å®¹
            if not any(pattern in text.lower() for pattern in [
                'transcript & subtitles', 'youtube transcript', 'bilibili subtitles',
                'video downloader', 'text to speech', 'ai-powered video',
                'copyright', 'privacy policy', 'terms & conditions',
                'chrome web store', 'edge add-ons', 'thumbnail downloader'
            ]):
                cleaned_subtitles.append(text)
        
        logger.info(f"æœ€ç»ˆæå–åˆ° {len(cleaned_subtitles)} æ®µçº¯å‡€å­—å¹•å†…å®¹")
        return cleaned_subtitles
    
    def save_subtitles(self, result: Dict, output_file: str = None) -> str:
        """ä¿å­˜å­—å¹•åˆ°æ–‡ä»¶"""
        if not result['success']:
            logger.error("æ— æ³•ä¿å­˜å­—å¹•ï¼šæå–å¤±è´¥")
            return None
            
        if not output_file:
            video_id = result.get('video_id', 'unknown')
            output_file = f"subtitles_{video_id}_{int(time.time())}.txt"
        
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write("="*60 + "\n")
                f.write("Bilibiliè§†é¢‘å­—å¹•æå–ç»“æœ\n")
                f.write("="*60 + "\n")
                f.write(f"è§†é¢‘URL: {result['video_url']}\n")
                f.write(f"ç»“æœé¡µé¢: {result['result_url']}\n")
                f.write(f"è§†é¢‘ID: {result['video_id']}\n")
                f.write(f"æå–æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"å­—å¹•æ®µæ•°: {result['subtitle_count']}\n")
                f.write("="*60 + "\n\n")
                
                for i, subtitle in enumerate(result['subtitles'], 1):
                    f.write(f"[{i:03d}] {subtitle}\n\n")
            
            logger.info(f"âœ… å­—å¹•å·²ä¿å­˜åˆ°: {output_file}")
            return output_file
            
        except Exception as e:
            logger.error(f"ä¿å­˜å­—å¹•æ–‡ä»¶å¤±è´¥: {str(e)}")
            return None
    

    
    def process_video(self, video_url: str) -> Dict:
        """å¤„ç†è§†é¢‘ï¼šæå–å­—å¹•"""
        logger.info(f"å¼€å§‹å¤„ç†è§†é¢‘: {video_url}")
        
        # æ£€æµ‹å¹³å°
        platform = self.detect_video_platform(video_url)
        logger.info(f"æ£€æµ‹åˆ°è§†é¢‘å¹³å°: {platform}")
        
        if platform != 'bilibili':
            logger.warning(f"å½“å‰åªæ”¯æŒBilibiliå¹³å°ï¼Œæ£€æµ‹åˆ°çš„å¹³å°: {platform}")
            # å¯ä»¥æ‰©å±•æ”¯æŒå…¶ä»–å¹³å°
        
        # æå–å­—å¹•
        result = self.extract_bilibili_subtitles(video_url)
        
        if result['success']:
            # ä¿å­˜å­—å¹•
            subtitle_file = self.save_subtitles(result)
            result['subtitle_file'] = subtitle_file
            
            # æ˜¾ç¤ºç»“æœ
            self._display_results(result)
        
        return result
    
    def _display_results(self, result: Dict):
        """æ˜¾ç¤ºæå–ç»“æœ"""
        print("\n" + "="*80)
        print("ğŸ‰ è§†é¢‘å­—å¹•æå–å®Œæˆï¼")
        print("="*80)
        print(f"ğŸ“¹ è§†é¢‘URL: {result['video_url']}")
        print(f"ğŸ†” è§†é¢‘ID: {result['video_id']}")
        print(f"ğŸ“ å­—å¹•æ®µæ•°: {result['subtitle_count']}")
        print(f"ğŸ’¾ ä¿å­˜æ–‡ä»¶: {result.get('subtitle_file', 'æœªä¿å­˜')}")
        
        if result['subtitles']:
            print(f"\nğŸ“‹ å­—å¹•å†…å®¹é¢„è§ˆï¼ˆå‰5æ®µï¼‰:")
            print("-" * 80)
            for i, text in enumerate(result['subtitles'][:5], 1):
                print(f"{i:2d}. {text[:150]}{'...' if len(text) > 150 else ''}")
                print()
            
            if len(result['subtitles']) > 5:
                print(f"... è¿˜æœ‰ {len(result['subtitles']) - 5} æ®µå­—å¹•å†…å®¹")
        
        print("="*80)

def main():
    """ä¸»å‡½æ•°"""
    print("="*60)
    print("ğŸ¬ Bilibiliè§†é¢‘å­—å¹•æå–å™¨")
    print("="*60)
    
    # è·å–ç”¨æˆ·è¾“å…¥
    video_url = input("\nğŸ“¹ è¯·è¾“å…¥Bilibiliè§†é¢‘URL (å›è½¦ä½¿ç”¨é»˜è®¤æµ‹è¯•è§†é¢‘): ").strip()
    if not video_url:
        video_url = "https://www.bilibili.com/video/BV1DsnzzwEUF/"
        print(f"ä½¿ç”¨é»˜è®¤æµ‹è¯•è§†é¢‘: {video_url}")
    
    # åˆå§‹åŒ–æå–å™¨
    extractor = BilibiliSubtitleExtractor()
    
    # å¤„ç†è§†é¢‘
    print(f"\nğŸš€ å¼€å§‹å¤„ç†è§†é¢‘...")
    result = extractor.process_video(video_url)
    
    if result['success']:
        print(f"\nâœ… å­—å¹•æå–æˆåŠŸï¼å…±æå–åˆ° {result['subtitle_count']} æ®µå­—å¹•")
        print(f"ğŸ“ å­—å¹•æ–‡ä»¶å·²ä¿å­˜åˆ°: {result.get('subtitle_file')}")
    else:
        print(f"\nâŒ å­—å¹•æå–å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
    
    print("\n" + "="*60)

if __name__ == "__main__":
    main()